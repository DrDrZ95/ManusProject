# Unsloth + LoRA Fine-tuning Guide

This guide explains how to use the Unsloth library with LoRA (Low-Rank Adaptation) for efficient fine-tuning of large language models in the AI Agent project.

## Overview

Unsloth is a library that significantly speeds up LLM fine-tuning by optimizing the underlying operations. When combined with LoRA, which reduces the number of trainable parameters, you can fine-tune large models with minimal computational resources.

Key benefits:
- **Speed**: Up to 3x faster training compared to standard methods
- **Efficiency**: Requires less memory and compute resources
- **Quality**: Maintains or improves model quality with parameter-efficient fine-tuning

## Installation

To set up the Unsloth + LoRA fine-tuning environment:

```bash
cd /path/to/ai-agent
chmod +x finetune/install_dependencies.sh
./finetune/install_dependencies.sh
source finetune/venv/bin/activate
```

This script installs all necessary dependencies including PyTorch, Transformers, Unsloth, and related libraries in a dedicated virtual environment.

## Directory Structure

```
ai-agent/
└── finetune/
    ├── install_dependencies.sh  # Installation script
    ├── utils.py                 # Utility functions for fine-tuning
    ├── venv/                    # Virtual environment (created by install script)
    ├── requirements.txt         # Generated by install script
    └── examples/                # Example fine-tuning scripts (to be added)
```

## Basic Usage

The `utils.py` module provides a comprehensive set of functions for fine-tuning. Here's a basic example:

```python
from finetune.utils import FineTuningConfig, run_fine_tuning

# Configure fine-tuning parameters
config = FineTuningConfig(
    model_name="Qwen/Qwen2-7B-Instruct",  # Base model to fine-tune
    output_dir="./results",               # Where to save results
    lora_r=16,                            # LoRA rank
    batch_size=4,                         # Batch size per device
    num_train_epochs=3,                   # Number of training epochs
    learning_rate=2e-4,                   # Learning rate
    max_seq_length=2048,                  # Maximum sequence length
)

# Run fine-tuning
model_path = run_fine_tuning(
    data_path="path/to/your/dataset.json",  # Path to your dataset
    config=config,
    train_test_split=0.1,                   # 10% of data for evaluation
)

print(f"Fine-tuned model saved to: {model_path}")
```

## Dataset Format

Your dataset should be in one of these formats:

1. **JSON/JSONL**: Each example should have `instruction` and `response` fields (or `prompt`/`completion` or `input`/`output`).

```json
{
  "instruction": "Explain the concept of fine-tuning.",
  "response": "Fine-tuning is the process of taking a pre-trained model and further training it on a specific dataset to adapt it for a particular task or domain."
}
```

2. **CSV**: Similar structure with columns for instructions and responses.

## Advanced Configuration

The `FineTuningConfig` class provides many parameters to customize your fine-tuning:

- **Model parameters**: model_name, max_seq_length
- **LoRA parameters**: lora_r, lora_alpha, lora_dropout
- **Training parameters**: learning_rate, batch_size, num_train_epochs, etc.
- **Logging and saving**: save_steps, logging_steps, eval_steps
- **Precision**: fp16, bf16
- **Tracking**: use_wandb, wandb_project, wandb_run_name

## Customizing the Prompt Template

You can customize how your data is formatted for training by providing a prompt template:

```python
prompt_template = """<|im_start|>system
You are a helpful AI assistant specialized in {domain}.<|im_end|>
<|im_start|>user
{instruction}<|im_end|>
<|im_start|>assistant
{response}<|im_end|>"""

# Use this template in run_fine_tuning
run_fine_tuning(
    data_path="path/to/your/dataset.json",
    config=config,
    prompt_template=prompt_template,
)
```

## Generating Text with Your Fine-tuned Model

After fine-tuning, you can generate text using your model:

```python
from finetune.utils import load_model_and_tokenizer, generate_text

# Load your fine-tuned model
model, tokenizer = load_model_and_tokenizer(
    FineTuningConfig(model_name="./results/final_model")
)

# Generate text
prompt = "Explain the benefits of LoRA fine-tuning."
generated_text = generate_text(
    model=model,
    tokenizer=tokenizer,
    prompt=prompt,
    max_new_tokens=512,
    temperature=0.7,
)

print(generated_text)
```

## Integration with the AI Agent

The fine-tuned models can be integrated with the existing AI Agent infrastructure:

1. Fine-tune a model using the utilities provided
2. Save the model to a location accessible by the Python model server
3. Update the model server configuration to use your fine-tuned model
4. Restart the model server to apply changes

## Troubleshooting

- **Out of memory errors**: Reduce batch_size, max_seq_length, or use a smaller model
- **Slow training**: Ensure you're using GPU acceleration and have properly installed CUDA
- **Poor results**: Try adjusting learning_rate, num_train_epochs, or lora_r parameters

## Next Steps

- Create custom datasets for your specific use case
- Experiment with different base models (Llama, Mistral, etc.)
- Try different LoRA configurations to optimize for your hardware
- Implement evaluation metrics specific to your application

## References

- [Unsloth GitHub Repository](https://github.com/unslothai/unsloth)
- [LoRA Paper](https://arxiv.org/abs/2106.09685)
- [PEFT Documentation](https://huggingface.co/docs/peft/index)
- [Transformers Documentation](https://huggingface.co/docs/transformers/index)
